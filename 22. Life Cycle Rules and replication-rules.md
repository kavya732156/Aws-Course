***Life Cycle Rules***<br>
S3 Lifecycle rules automatically transition or delete objects over time to optimize cost and manage data retention.<br>
Versioning is not mandatory to enable on the S3 bucket, but if the bucket is versioned then we need to create a separate life cycle rule for the "non-current versions"<br>
Sequence of the Storage classed for life cycle rule<br>
Standard, Standard-IA, Intelligent-Tier, OneZone-IA, Glacier Instant Retrieval, Glacier Flexible Retrieval, Glacier Deep Archive and Expire

***steps***<br>
1) Create S3 Bucket + Enable Versioning (N. Virginia)<br>
2) Create Lifecycle Rule (As Per Requirement)<br>
   Go to: S3 → lifecyclerulesmarch-11th → Management → Lifecycle rules → Create rule<br>
   Rule name: lifecycle-transition-rule<br>
   Scope: Apply to all objects (or choose prefix if you want)<br>

***Note:  Lifecycle rules run once per day***
Current Version of the object:
--------------------------------------
Standrad --30 Days--> Standard-IA --60 Days--> OneZone-IA  --90 Days--> Glacier Flexible Retrieval --> 180 days--> Glacier Deep Archive --365 Days--> Expire <br>

***Previous Version of the object***:<br>
Standard --30 Days--> Glacier Deep Archive  -- 365 Days--> Expire

***S3 Replication Rules***<br>
1. Automatically copies objects from one bucket to another
2. Used for DR (disaster recovery), compliance, and latency
3. equires Versioning enabled on both source & destination buckets
4. Two types: CRR (Cross-Region Replication) and SRR (Same-Region Replication)
5. Replicates only new objects by default
6. Can filter by prefix or object tags
7. Supports replicating object metadata & encryption (SSE-S3 / SSE-KMS)
8. Can replicate delete markers (optional)
9. Replication costs extra (data transfer + storage)<br>

***CRR vs SRR***
```
| Feature  | CRR                            | SRR                          |
| -------- | ------------------------------ | ---------------------------- |
| Region   | Different regions              | Same region                  |
| Use case | DR, compliance                 | Log aggregation, data copies |
| Latency  | Higher                         | Lower                        |
| Cost     | Higher (inter-region transfer) | Lower                        |
```
***Steps***
```
Enable Versioning on source & destination buckets
Go to Source bucket → Management → Replication rules → Create rule
Choose Destination bucket (same or different region)
Select Scope (all objects / prefix / tags)
Choose IAM role (auto-create is fine)
(Optional) Enable replicate delete markers
Save
```
***1. Entity Tag (ETag) in S3***<br>
ETag (Entity Tag) is a unique identifier (usually a hash) generated by S3 for an object to represent the object’s content.<br>


***Purpose of ETag***<br>
1. Checks data integrity (verify upload/download correctness)
2. Detects changes in an object (ETag changes when content changes)
3. Helps with cache validation (If-None-Match / If-Match headers)
4. Used by tools to confirm multipart upload parts
5. Helps ensure file consistency during transfers
6. Example: When you upload a file to S3, you get a response with an ETag:
```
{
  "ETag": "\"5d41402abc4b2a76b9719d911017c592\""
}
Here, the value (5d41402abc4b2a76b9719d911017c592) is an MD5 hash of the object.
```
Important Note: For multipart uploads, the ETag is not a simple MD5 but a combination of parts and their checksums<br>

***S3 URI — Simple Definition***<br>
S3 URI is a special format used to identify the location of a bucket or an object inside S3, mainly for programmatic access and CLI/SDK operations.<br>
Examples: s3://my-bucket and s3://my-bucket/folder/file.txt <br>

***Purpose of S3 URI***
1. Used by AWS CLI, SDKs, and services (not in browsers)
2. Identifies exact bucket and object path
3. Common in commands like: aws s3 cp s3://bucket/file.txt ./ and aws s3 sync ./data s3://bucket/data
4. Works with IAM permissions
5. Makes scripts and automation simple and readable

***S3 URI vs Object URL***
```
| Feature         | S3 URI            | Object URL                            |
| --------------- | ----------------- | ------------------------------------- |
| Used in browser |   No              |   Yes                                 |
| Used in CLI/SDK |   Yes             |   No                                  |
| Format          | `s3://bucket/key` | `https://bucket.s3.amazonaws.com/key` |
| Needs auth      |   Usually         |   Public objects only                 |
```
***Here are the advantages of Amazon S3***<br>
1. Highly scalable – store unlimited data, no capacity planning
2. 11 9s durability (99.999999999%) – extremely reliable storage
3. Global availability – accessible from anywhere
4. Cost-effective – multiple storage classes to reduce cost
5. Strong security – IAM, bucket policies, encryption, Object Lock
6. High availability – data replicated across multiple AZs
7. High performance – fast uploads/downloads at scale
8. Easy integration – works with many AWS services
9. Lifecycle management – automate transitions & deletion
10. Backup & DR friendly – versioning, replication, cross-region DR
11. Static website hosting – host websites directly from S3
12. Event notifications – trigger Lambda/SQS/SNS on object events <br>

***Use Cases for Amazon S3 <br>***

1. Backup & Restore – store backups of servers, databases, and apps
2. Disaster Recovery (DR) – replicate data to another region for DR
3. Static Website Hosting – host HTML/CSS/JS websites
4. Media Storage & Delivery – images, videos, audio files
5. Data Archiving – long-term storage using Glacier classes
6. Data Lakes – central storage for analytics & big data
7. Content Distribution – integrate with CDN for faster delivery
8. Log Storage – application logs, access logs
9. Machine Learning datasets – training data storage
10. App data storage – store user uploads, documents
11. Cross-account sharing – share data between teams/accounts
12. Dev/Test storage – artifacts, builds, test data
